{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "def cleaning(text):\n",
    "    text = text.lower()\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    clean = re.compile('<.*?>')\n",
    "    text = re.sub(clean,'',text)\n",
    "    text = pattern.sub('', text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)        \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text) \n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"did't\", \"did not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"have't\", \"have not\", text)\n",
    "\n",
    "    text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i recently stayed at the hyatt regency in downtown chicago my stay here was wonderful thanks to such a friendly staff i needed to do some research for work and the free wifi made this very easy the bed was so nice and soft even the pillows were fluffy i was able to put my valuables in the inroom safe the local restaurants were amazing i will recommend this to my friends and i am staying here again in the future']\n"
     ]
    }
   ],
   "source": [
    "### the string for testing\n",
    "s = \"I recently stayed at the Hyatt Regency in downtown Chicago. My stay here was wonderful thanks to such a friendly staff. I needed to do some research for work and the free Wi-Fi made this very easy. The bed was so nice and soft! Even the pillows were fluffy. I was able to put my valuables in the in-room safe. The local restaurants were amazing. I will recommend this to my friends and I am staying here again in the future.\"\n",
    "s = [cleaning(s)]          ## cleaned text is returned to a list having a string\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading the tokenizer which tokenizes the sentences.\n",
    "tokenizer = joblib.load(\"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 206, 53, 16, 2, 357, 583, 8, 172, 25, 17, 31, 76, 7, 170, 924, 4, 277, 6, 110, 42, 5, 362, 4, 81, 93, 2594, 12, 184, 3, 2, 174, 352, 105, 22, 27, 464, 2, 78, 7, 41, 75, 3, 1014, 79, 2, 386, 23, 1728, 5, 7, 267, 4, 427, 17, 3548, 8, 2, 845, 863, 2, 909, 292, 23, 234, 5, 55, 115, 22, 4, 17, 364, 3, 5, 108, 118, 76, 63, 8, 2, 765]]\n"
     ]
    }
   ],
   "source": [
    "#### convert it into sequences\n",
    "s = tokenizer.texts_to_sequences(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 80)\n"
     ]
    }
   ],
   "source": [
    "## pad the sequence\n",
    "\n",
    "s = pad_sequences(s,maxlen=80,padding='post')\n",
    "print(s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the model\n",
    "model = load_model(\"reviews.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000022D65E033A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000022D65E033A8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[0.14823978 0.8517602 ]\n"
     ]
    }
   ],
   "source": [
    "## predict the text\n",
    "res = model.predict(s,batch_size=1)   ### batch size=1 as there is only one test sentence\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review is genuine.\n"
     ]
    }
   ],
   "source": [
    "### 0: fake and 1: genuine\n",
    "\n",
    "if res[0][0]>res[0][1]:\n",
    "    print(\"The review is fake.\")\n",
    "else:\n",
    "    print(\"The review is genuine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
